{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMKePTxUQnOUAI41487IdeA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "721687f4061d4b1da8656718323a09bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b0b495832164b3c8e5c0cf31af9b375",
              "IPY_MODEL_c30cf81502d048179b50d76c283c8132",
              "IPY_MODEL_d91facd6e60740809638c2fec2a422a3"
            ],
            "layout": "IPY_MODEL_53f7c847819a4f288951c42c5c3dd751"
          }
        },
        "9b0b495832164b3c8e5c0cf31af9b375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae5ea1afda064659b5c3804df6dad9e9",
            "placeholder": "​",
            "style": "IPY_MODEL_5646a08a175f479288b7bc71cffb5c3e",
            "value": "model.safetensors: 100%"
          }
        },
        "c30cf81502d048179b50d76c283c8132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebf0b3a918cd4a9bbb5ff18d0735d105",
            "max": 267954768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e91295a1e3c405eb93ee3c08e271925",
            "value": 267954768
          }
        },
        "d91facd6e60740809638c2fec2a422a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_564af6f49f864c49a5ac857568020f6b",
            "placeholder": "​",
            "style": "IPY_MODEL_f318d55a61234e93bc3ec619ab0bd286",
            "value": " 268M/268M [00:01&lt;00:00, 96.8MB/s]"
          }
        },
        "53f7c847819a4f288951c42c5c3dd751": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae5ea1afda064659b5c3804df6dad9e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5646a08a175f479288b7bc71cffb5c3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebf0b3a918cd4a9bbb5ff18d0735d105": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e91295a1e3c405eb93ee3c08e271925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "564af6f49f864c49a5ac857568020f6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f318d55a61234e93bc3ec619ab0bd286": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7lAYp5ZD8s5"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
        "# Source for \"Build a Large Language Model From Scratch\"\n",
        "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
        "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
        "\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256, use_attention_mask=False):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.max_length = max_length if max_length is not None else self._longest_encoded_length(tokenizer)\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.use_attention_mask = use_attention_mask\n",
        "\n",
        "        # Pre-tokenize texts and create attention masks if required\n",
        "        self.encoded_texts = [\n",
        "            tokenizer.encode(text, truncation=True, max_length=self.max_length)\n",
        "            for text in self.data[\"text\"]\n",
        "        ]\n",
        "        self.encoded_texts = [\n",
        "            et + [pad_token_id] * (self.max_length - len(et))\n",
        "            for et in self.encoded_texts\n",
        "        ]\n",
        "\n",
        "        if self.use_attention_mask:\n",
        "            self.attention_masks = [\n",
        "                self._create_attention_mask(et)\n",
        "                for et in self.encoded_texts\n",
        "            ]\n",
        "        else:\n",
        "            self.attention_masks = None\n",
        "\n",
        "    def _create_attention_mask(self, encoded_text):\n",
        "        # fgs sole purpose to id text from padding\n",
        "        return [1 if token_id != self.pad_token_id else 0 for token_id in encoded_text]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoded = self.encoded_texts[index]\n",
        "        label = self.data.iloc[index][\"label\"]\n",
        "\n",
        "        if self.use_attention_mask: attention_mask = self.attention_masks[index]\n",
        "        else: attention_mask = torch.ones(self.max_length, dtype=torch.long)\n",
        "\n",
        "        return (\n",
        "            torch.tensor(encoded, dtype=torch.long),\n",
        "            torch.tensor(attention_mask, dtype=torch.long),\n",
        "            torch.tensor(label, dtype=torch.long)\n",
        "            )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _longest_encoded_length(self, tokenizer):\n",
        "        max_length = 0\n",
        "        for text in self.data[\"text\"]:\n",
        "            encoded_length = len(tokenizer.encode(text))\n",
        "            if encoded_length > max_length:\n",
        "                max_length = encoded_length\n",
        "        return max_length\n"
      ],
      "metadata": {
        "id": "obz0AtIzEHRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fgs: The fact that the IMDBDataset class returns a tuple of (encoded_text, attention_mask, label) is not a strict, universal requirement imposed by Hugging Face models, but it is a very common and highly recommended practice when working with padded sequences and models that utilize attention masks, including many models from the Hugging Face library.\n",
        "\n",
        "Here's why it's a good practice:\n",
        "\n",
        "1. Hugging Face Model Input: Many Hugging Face models, especially those designed for sequence tasks like classification, accept input_ids (the encoded text) and attention_mask as separate inputs. Providing the attention mask allows the model to correctly handle the padded tokens and focus its attention only on the real content of the sequence.\n",
        "2. Handling Padding: When you have sequences of varying lengths and pad them to a fixed maximum length, you need a way to tell the model which parts are real data and which are just padding. The attention mask serves this purpose. If you didn't provide an attention mask, the model might incorrectly process the padding tokens as if they were meaningful input, leading to degraded performance.\n",
        "3. PyTorch DataLoader Compatibility: PyTorch DataLoader is designed to work with datasets that return tensors. By returning a tuple of tensors, your custom dataset seamlessly integrates with the DataLoader, which will then batch these tensors together for efficient processing.\n",
        "4. Clarity and Organization: Returning the input, attention mask, and label as separate elements in a tuple makes the data structure clear and easy to work with in your training loop. You can easily unpack the tuple to get the individual components needed for your mode"
      ],
      "metadata": {
        "id": "fkgBw7WZNOEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! [ ! -f download_prepare_dataset.py ] && wget https://raw.githubusercontent.com/giordafrancis/llms_from_scratch/refs/heads/main/ch06/03_bonus_imdb-classification/download_prepare_dataset.py\n"
      ],
      "metadata": {
        "id": "rDRD4z92-inS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python download_prepare_dataset.py"
      ],
      "metadata": {
        "id": "5Eq5KiGgKkLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv *.csv sample_data/"
      ],
      "metadata": {
        "id": "Da3j2IOnK-5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "gCORS5nnLFfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fgs: just testing the data loaders, as i'm interested to understand how the data looks like"
      ],
      "metadata": {
        "id": "t6ulGB2IW9ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "# Instantiate dataloaders\n",
        "###############################\n",
        "\n",
        "base_path = Path(\"sample_data/\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "use_attention_mask = True\n",
        "# use_attention_mask = False\n",
        "train_dataset = IMDBDataset(\n",
        "    base_path / \"train.csv\",\n",
        "    max_length=256,\n",
        "    tokenizer=tokenizer,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    use_attention_mask=use_attention_mask\n",
        ")\n",
        "val_dataset = IMDBDataset(\n",
        "    base_path / \"validation.csv\",\n",
        "    max_length=256,\n",
        "    tokenizer=tokenizer,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    use_attention_mask=use_attention_mask\n",
        ")\n",
        "test_dataset = IMDBDataset(\n",
        "    base_path / \"test.csv\",\n",
        "    max_length=256,\n",
        "    tokenizer=tokenizer,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    use_attention_mask=use_attention_mask\n",
        ")\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "VIAoJERoJvZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X and y\n",
        "\n",
        "# Set options to display more content in pandas columns\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "train_dataset.data.sample(10)"
      ],
      "metadata": {
        "id": "2Xw6JlrhVSzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for b in train_loader: break\n",
        "# fgs: N X (token_ids, padd attention, target)\n",
        "b"
      ],
      "metadata": {
        "id": "3alZ8YeLLdOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, attention_mask_batch, target_batch, model, device):\n",
        "    attention_mask_batch = attention_mask_batch.to(device)\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    # logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
        "    logits = model(input_batch, attention_mask=attention_mask_batch).logits # fgs padding att mask used, so need to pass this to model\n",
        "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "eyxCKAWWEfi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Same as in chapter 5\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if num_batches is None: num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, attention_mask_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, attention_mask_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "@torch.no_grad()  # Disable gradient tracking for efficiency\n",
        "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
        "    model.eval()\n",
        "    correct_predictions, num_examples = 0, 0\n",
        "\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, attention_mask_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            attention_mask_batch = attention_mask_batch.to(device)\n",
        "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "            # logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
        "            logits = model(input_batch, attention_mask=attention_mask_batch).logits\n",
        "            predicted_labels = torch.argmax(logits, dim=1)\n",
        "            num_examples += predicted_labels.shape[0]\n",
        "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
        "        else:\n",
        "            break\n",
        "    return correct_predictions / num_examples\n",
        "\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                            eval_freq, eval_iter, max_steps=None):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    examples_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, attention_mask_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, attention_mask_batch, target_batch, model, device)\n",
        "            loss.backward()  # Calculate loss gradients\n",
        "            optimizer.step()  # Update model weights using loss gradients\n",
        "            examples_seen += input_batch.shape[0]  # New: track examples instead of tokens\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "            if max_steps is not None and global_step > max_steps:\n",
        "                break\n",
        "\n",
        "        # New: Calculate accuracy after each epoch\n",
        "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(val_accuracy)\n",
        "\n",
        "        if max_steps is not None and global_step > max_steps: break\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs, examples_seen\n",
        "\n"
      ],
      "metadata": {
        "id": "kw8hWgiSGdh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define training params"
      ],
      "metadata": {
        "id": "lgh08AAYZVlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"distilbert\"\n",
        "trainable_layers = \"all\"\n",
        "use_attention_mask = True\n",
        "num_epochs = 1\n",
        "learning_rate = 5e-6\n",
        "\n",
        "# data loaders params\n",
        "num_workers = 0\n",
        "batch_size = 8"
      ],
      "metadata": {
        "id": "6BO8UAbyRBgN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"modernbert-base\"\n",
        "trainable_layers = \"last_block\"\n",
        "use_attention_mask = True\n",
        "num_epochs = 3\n",
        "learning_rate = 5e-6\n",
        "\n",
        "# data loaders params\n",
        "num_workers = 0\n",
        "batch_size = 8"
      ],
      "metadata": {
        "id": "x4NugRpzXjFw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model selection"
      ],
      "metadata": {
        "id": "sEODujaJa6rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cUcx1f1JsnQG",
        "outputId": "c27a5079-fbf8-4efd-b037-b2350dfff5c8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'distilbert'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "if model == \"distilbert\":\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            \"distilbert-base-uncased\", num_labels=2)\n",
        "  model.out_head = torch.nn.Linear(in_features=768, out_features=2)\n",
        "\n",
        "  for param in model.parameters(): param.requires_grad = False\n",
        "  if trainable_layers == \"last_layer\":\n",
        "    for param in model.out_head.parameters(): param.requires_grad = True\n",
        "  elif trainable_layers == \"last_block\":\n",
        "      for param in model.pre_classifier.parameters(): param.requires_grad = True\n",
        "      for param in model.distilbert.transformer.layer[-1].parameters(): param.requires_grad = True\n",
        "  elif trainable_layers == \"all\":\n",
        "      for param in model.parameters(): param.requires_grad = True\n",
        "  else:\n",
        "      raise ValueError(\"Invalid --trainable_layers argument.\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "elif model == \"bert\":\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\n",
        "           \"bert-base-uncased\", num_labels=2)\n",
        "  model.classifier = torch.nn.Linear(in_features=768, out_features=2)\n",
        "  for param in model.parameters(): param.requires_grad = False\n",
        "  if trainable_layers == \"last_layer\":\n",
        "    for param in model.classifier.parameters(): param.requires_grad = True\n",
        "  elif trainable_layers == \"last_block\":\n",
        "      for param in model.classifier.parameters(): param.requires_grad = True\n",
        "      for param in model.bert.pooler.dense.parameters(): param.requires_grad = True\n",
        "      for param in model.bert.encoder.layer[-1].parameters(): param.requires_grad = True\n",
        "  elif trainable_layers == \"all\":\n",
        "    for param in model.parameters(): param.requires_grad = True\n",
        "  else:\n",
        "      raise ValueError(\"Invalid --trainable_layers argument.\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "elif model == \"roberta\":\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            \"FacebookAI/roberta-large\", num_labels=2)\n",
        "  model.classifier.out_proj = torch.nn.Linear(in_features=1024, out_features=2)\n",
        "  for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "  if trainable_layers == \"last_layer\":\n",
        "      for param in model.classifier.parameters():\n",
        "          param.requires_grad = True\n",
        "  elif trainable_layers == \"last_block\":\n",
        "      for param in model.classifier.parameters():\n",
        "          param.requires_grad = True\n",
        "      for param in model.roberta.encoder.layer[-1].parameters():\n",
        "          param.requires_grad = True\n",
        "  elif trainable_layers == \"all\":\n",
        "      for param in model.parameters():\n",
        "          param.requires_grad = True\n",
        "  else:\n",
        "      raise ValueError(\"Invalid --trainable_layers argument.\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-large\")\n",
        "\n",
        "elif model in (\"modernbert-base\", \"modernbert-large\"):\n",
        "  if model == \"modernbert-base\":\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            \"answerdotai/ModernBERT-base\", num_labels=2\n",
        "        )\n",
        "        model.classifier = torch.nn.Linear(in_features=768, out_features=2)\n",
        "  else:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            \"answerdotai/ModernBERT-large\", num_labels=2\n",
        "        )\n",
        "        model.classifier = torch.nn.Linear(in_features=1024, out_features=2)\n",
        "  for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "  if trainable_layers == \"last_layer\":\n",
        "      for param in model.classifier.parameters():\n",
        "          param.requires_grad = True\n",
        "  elif trainable_layers == \"last_block\":\n",
        "      for param in model.classifier.parameters():\n",
        "          param.requires_grad = True\n",
        "      for param in model.model.layers[-1].parameters():\n",
        "          param.requires_grad = True\n",
        "      for param in model.head.parameters():\n",
        "          param.requires_grad = True\n",
        "      # for param in model.classifier.parameters():\n",
        "      #     param.requires_grad = True\n",
        "  elif trainable_layers == \"all\":\n",
        "      for param in model.parameters():\n",
        "          param.requires_grad = True\n",
        "  else:\n",
        "      raise ValueError(\"Invalid --trainable_layers argument.\")\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
        "\n",
        "elif model == \"deberta-v3-base\":\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"microsoft/deberta-v3-base\", num_labels=2)\n",
        "  model.classifier = torch.nn.Linear(in_features=768, out_features=2)\n",
        "  for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "  if trainable_layers == \"last_layer\":\n",
        "      for param in model.classifier.parameters():\n",
        "          param.requires_grad = True\n",
        "  elif trainable_layers == \"last_block\":\n",
        "    for param in model.classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.pooler.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.deberta.encoder.layer[-1].parameters():\n",
        "        param.requires_grad = True\n",
        "  elif trainable_layers == \"all\":\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = True\n",
        "  else:\n",
        "      raise ValueError(\"Invalid --trainable_layers argument.\")\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "\n",
        "else:\n",
        "  raise ValueError(\"Selected --model {model} not supported.\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711,
          "referenced_widgets": [
            "721687f4061d4b1da8656718323a09bc",
            "9b0b495832164b3c8e5c0cf31af9b375",
            "c30cf81502d048179b50d76c283c8132",
            "d91facd6e60740809638c2fec2a422a3",
            "53f7c847819a4f288951c42c5c3dd751",
            "ae5ea1afda064659b5c3804df6dad9e9",
            "5646a08a175f479288b7bc71cffb5c3e",
            "ebf0b3a918cd4a9bbb5ff18d0735d105",
            "6e91295a1e3c405eb93ee3c08e271925",
            "564af6f49f864c49a5ac857568020f6b",
            "f318d55a61234e93bc3ec619ab0bd286"
          ]
        },
        "id": "vklD4ooHZbP2",
        "outputId": "ca340fb8-f795-4f19-d922-b8096742c8ba"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "721687f4061d4b1da8656718323a09bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and Data Loading"
      ],
      "metadata": {
        "id": "-MlfYJ0WbUTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = Path(\"sample_data/\")\n",
        "\n",
        "train_dataset = IMDBDataset(\n",
        "    base_path / \"train.csv\",\n",
        "    max_length=256,\n",
        "    tokenizer=tokenizer,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    use_attention_mask=use_attention_mask\n",
        ")\n",
        "val_dataset = IMDBDataset(\n",
        "    base_path / \"validation.csv\",\n",
        "    max_length=256,\n",
        "    tokenizer=tokenizer,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    use_attention_mask=use_attention_mask\n",
        ")\n",
        "test_dataset = IMDBDataset(\n",
        "    base_path / \"test.csv\",\n",
        "    max_length=256,\n",
        "    tokenizer=tokenizer,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    use_attention_mask=use_attention_mask\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "XapdkO5WbYkX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "VOX8nT8zbE0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "# Train model\n",
        "###############################\n",
        "\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr= learning_rate,\n",
        "                              weight_decay=0.1)\n",
        "\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=50, eval_iter=20,\n",
        "    max_steps=None\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nlMiA8jJs0k",
        "outputId": "87043a29-b6a6-43dd-d6de-e407ae09133c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 0.694, Val loss 0.693\n",
            "Ep 1 (Step 000050): Train loss 0.677, Val loss 0.677\n",
            "Ep 1 (Step 000100): Train loss 0.626, Val loss 0.607\n",
            "Ep 1 (Step 000150): Train loss 0.409, Val loss 0.416\n",
            "Ep 1 (Step 000200): Train loss 0.390, Val loss 0.310\n",
            "Ep 1 (Step 000250): Train loss 0.420, Val loss 0.390\n",
            "Ep 1 (Step 000300): Train loss 0.252, Val loss 0.272\n",
            "Ep 1 (Step 000350): Train loss 0.319, Val loss 0.272\n",
            "Ep 1 (Step 000400): Train loss 0.321, Val loss 0.259\n",
            "Ep 1 (Step 000450): Train loss 0.230, Val loss 0.250\n",
            "Ep 1 (Step 000500): Train loss 0.272, Val loss 0.280\n",
            "Ep 1 (Step 000550): Train loss 0.291, Val loss 0.260\n",
            "Ep 1 (Step 000600): Train loss 0.208, Val loss 0.228\n",
            "Ep 1 (Step 000650): Train loss 0.257, Val loss 0.238\n",
            "Ep 1 (Step 000700): Train loss 0.339, Val loss 0.237\n",
            "Ep 1 (Step 000750): Train loss 0.280, Val loss 0.220\n",
            "Ep 1 (Step 000800): Train loss 0.257, Val loss 0.217\n",
            "Ep 1 (Step 000850): Train loss 0.250, Val loss 0.218\n",
            "Ep 1 (Step 000900): Train loss 0.193, Val loss 0.243\n",
            "Ep 1 (Step 000950): Train loss 0.307, Val loss 0.226\n",
            "Ep 1 (Step 001000): Train loss 0.259, Val loss 0.218\n",
            "Ep 1 (Step 001050): Train loss 0.202, Val loss 0.225\n",
            "Ep 1 (Step 001100): Train loss 0.299, Val loss 0.217\n",
            "Ep 1 (Step 001150): Train loss 0.199, Val loss 0.236\n",
            "Ep 1 (Step 001200): Train loss 0.186, Val loss 0.232\n",
            "Ep 1 (Step 001250): Train loss 0.180, Val loss 0.223\n",
            "Ep 1 (Step 001300): Train loss 0.227, Val loss 0.255\n",
            "Ep 1 (Step 001350): Train loss 0.311, Val loss 0.217\n",
            "Ep 1 (Step 001400): Train loss 0.191, Val loss 0.204\n",
            "Ep 1 (Step 001450): Train loss 0.259, Val loss 0.218\n",
            "Ep 1 (Step 001500): Train loss 0.214, Val loss 0.205\n",
            "Ep 1 (Step 001550): Train loss 0.213, Val loss 0.198\n",
            "Ep 1 (Step 001600): Train loss 0.239, Val loss 0.229\n",
            "Ep 1 (Step 001650): Train loss 0.245, Val loss 0.243\n",
            "Ep 1 (Step 001700): Train loss 0.235, Val loss 0.226\n",
            "Ep 1 (Step 001750): Train loss 0.240, Val loss 0.195\n",
            "Ep 1 (Step 001800): Train loss 0.189, Val loss 0.213\n",
            "Ep 1 (Step 001850): Train loss 0.286, Val loss 0.209\n",
            "Ep 1 (Step 001900): Train loss 0.394, Val loss 0.243\n",
            "Ep 1 (Step 001950): Train loss 0.235, Val loss 0.213\n",
            "Ep 1 (Step 002000): Train loss 0.287, Val loss 0.190\n",
            "Ep 1 (Step 002050): Train loss 0.289, Val loss 0.187\n",
            "Ep 1 (Step 002100): Train loss 0.259, Val loss 0.194\n",
            "Ep 1 (Step 002150): Train loss 0.245, Val loss 0.192\n",
            "Ep 1 (Step 002200): Train loss 0.251, Val loss 0.220\n",
            "Ep 1 (Step 002250): Train loss 0.199, Val loss 0.194\n",
            "Ep 1 (Step 002300): Train loss 0.153, Val loss 0.188\n",
            "Ep 1 (Step 002350): Train loss 0.242, Val loss 0.184\n",
            "Ep 1 (Step 002400): Train loss 0.208, Val loss 0.197\n",
            "Ep 1 (Step 002450): Train loss 0.257, Val loss 0.194\n",
            "Ep 1 (Step 002500): Train loss 0.158, Val loss 0.214\n",
            "Ep 1 (Step 002550): Train loss 0.230, Val loss 0.191\n",
            "Ep 1 (Step 002600): Train loss 0.224, Val loss 0.276\n",
            "Ep 1 (Step 002650): Train loss 0.152, Val loss 0.192\n",
            "Ep 1 (Step 002700): Train loss 0.275, Val loss 0.202\n",
            "Ep 1 (Step 002750): Train loss 0.201, Val loss 0.204\n",
            "Ep 1 (Step 002800): Train loss 0.220, Val loss 0.200\n",
            "Ep 1 (Step 002850): Train loss 0.185, Val loss 0.206\n",
            "Ep 1 (Step 002900): Train loss 0.165, Val loss 0.207\n",
            "Ep 1 (Step 002950): Train loss 0.205, Val loss 0.208\n",
            "Ep 1 (Step 003000): Train loss 0.270, Val loss 0.208\n",
            "Ep 1 (Step 003050): Train loss 0.195, Val loss 0.200\n",
            "Ep 1 (Step 003100): Train loss 0.232, Val loss 0.202\n",
            "Ep 1 (Step 003150): Train loss 0.211, Val loss 0.205\n",
            "Ep 1 (Step 003200): Train loss 0.211, Val loss 0.214\n",
            "Ep 1 (Step 003250): Train loss 0.186, Val loss 0.206\n",
            "Ep 1 (Step 003300): Train loss 0.180, Val loss 0.253\n",
            "Ep 1 (Step 003350): Train loss 0.329, Val loss 0.291\n",
            "Ep 1 (Step 003400): Train loss 0.215, Val loss 0.207\n",
            "Ep 1 (Step 003450): Train loss 0.190, Val loss 0.204\n",
            "Ep 1 (Step 003500): Train loss 0.225, Val loss 0.201\n",
            "Ep 1 (Step 003550): Train loss 0.199, Val loss 0.232\n",
            "Ep 1 (Step 003600): Train loss 0.219, Val loss 0.257\n",
            "Ep 1 (Step 003650): Train loss 0.148, Val loss 0.206\n",
            "Ep 1 (Step 003700): Train loss 0.215, Val loss 0.203\n",
            "Ep 1 (Step 003750): Train loss 0.155, Val loss 0.221\n",
            "Ep 1 (Step 003800): Train loss 0.248, Val loss 0.201\n",
            "Ep 1 (Step 003850): Train loss 0.219, Val loss 0.205\n",
            "Ep 1 (Step 003900): Train loss 0.169, Val loss 0.183\n",
            "Ep 1 (Step 003950): Train loss 0.164, Val loss 0.192\n",
            "Ep 1 (Step 004000): Train loss 0.156, Val loss 0.176\n",
            "Ep 1 (Step 004050): Train loss 0.168, Val loss 0.186\n",
            "Ep 1 (Step 004100): Train loss 0.228, Val loss 0.183\n",
            "Ep 1 (Step 004150): Train loss 0.182, Val loss 0.192\n",
            "Ep 1 (Step 004200): Train loss 0.228, Val loss 0.180\n",
            "Ep 1 (Step 004250): Train loss 0.177, Val loss 0.229\n",
            "Ep 1 (Step 004300): Train loss 0.127, Val loss 0.187\n",
            "Ep 1 (Step 004350): Train loss 0.223, Val loss 0.178\n",
            "Training accuracy: 91.25% | Validation accuracy: 91.88%\n",
            "Training completed in 8.26 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model eval"
      ],
      "metadata": {
        "id": "b5GI7ztIds6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "# Evaluate model\n",
        "###############################\n",
        "\n",
        "print(\"\\nEvaluating on the full datasets ...\\n\")\n",
        "\n",
        "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU3kRfl2c87T",
        "outputId": "acf3a9d2-c279-4682-a317-fbb5d1424d05"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on the full datasets ...\n",
            "\n",
            "Training accuracy: 86.66%\n",
            "Validation accuracy: 86.80%\n",
            "Test accuracy: 86.10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "\n",
        "fgs: DistellBert seems a really good starting point model\n",
        "trainable_layers = \"last_block\"\n",
        "use_attention_mask = True\n",
        "num_epochs = 1\n",
        "learning_rate = 5e-6\n",
        "\n",
        "**DistillBert: Evaluating on the full datasets ...**\n",
        "\n",
        "- Training accuracy: 88.56%\n",
        "- Validation accuracy: 88.88%\n",
        "- Test accuracy: 88.84%\n",
        "\n",
        "\n",
        "**modernbert-base: Evaluating on the full datasets ...**\n",
        "epoch == 1\n",
        "\n",
        "- Training accuracy: 82.98%\n",
        "- Validation accuracy: 83.26%\n",
        "- Test accuracy: 82.36%\n",
        "\n",
        "epoch == 3\n",
        "\n",
        "Training accuracy: 86.66%\n",
        "Validation accuracy: 86.80%\n",
        "Test accuracy: 86.10%"
      ],
      "metadata": {
        "id": "XgZeg9tNfg6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCKnVkPmXrdn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}